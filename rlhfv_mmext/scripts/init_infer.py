#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
init_infer.py
å¤šæ¨¡æ€æ¨ç†è„šæœ¬ï¼ˆå›¾ç‰‡ + promptï¼‰ï¼š
- ä½¿ç”¨ CLIP æå–å›¾åƒç‰¹å¾ -> çº¿æ€§æŠ•å½± -> ä½œä¸º prefix_embeds è¾“å…¥ LLM
- æ”¯æŒå‘½ä»¤è¡ŒæŒ‡å®šå›¾ç‰‡å’Œ prompt
- æ”¯æŒ LoRA adapter
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel
from PIL import Image
import argparse
from peft import PeftModel

# ----------------------------
# é…ç½®
# ----------------------------
ROOT = os.path.expanduser("~/rlhfv")
LLM_PATH = os.path.join(ROOT, "models/open_llama_3b_v2")
CLIP_PATH = os.path.join(ROOT, "models/clip-vit-base-patch16")
PREFIX_TOKENS = 1  # ç”¨ 1 ä¸ª token è¡¨ç¤º image embedding
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------
# å‘½ä»¤è¡Œå‚æ•°
# ----------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--image", type=str, required=True, help="å›¾ç‰‡æ–‡ä»¶è·¯å¾„")
parser.add_argument("--prompt", type=str, required=True, help="æ–‡æœ¬æŒ‡ä»¤ prompt")
parser.add_argument("--lora", type=str, default=None, help="LoRA adapter ç›®å½•ï¼Œå¦‚æœæœ‰")
args = parser.parse_args()

image_path = os.path.expanduser(args.image)
prompt = args.prompt

if not os.path.exists(image_path):
    raise FileNotFoundError(f"å›¾ç‰‡ {image_path} ä¸å­˜åœ¨")

print("ğŸ–¼ï¸ å›¾åƒè·¯å¾„:", image_path)
print("ğŸ’¬ Prompt:", prompt)
print("ğŸš€ åŠ è½½ CLIP ä¸ LLM æ¨¡å‹...")

# ----------------------------
# åŠ è½½ CLIP
# ----------------------------
clip_model = CLIPModel.from_pretrained(CLIP_PATH).to(DEVICE)
clip_processor = CLIPProcessor.from_pretrained(CLIP_PATH)

# ----------------------------
# åŠ è½½ LLM
# ----------------------------
tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

llm = AutoModelForCausalLM.from_pretrained(LLM_PATH, torch_dtype=torch.float16 if DEVICE.type == "cuda" else torch.float32)
llm.to(DEVICE)

# å¦‚æœæœ‰ LoRA adapter
if args.lora and os.path.exists(args.lora):
    print("âš¡ åŠ è½½ LoRA adapter...")
    llm = PeftModel.from_pretrained(llm, args.lora)
else:
    print("âš ï¸ æœªæ‰¾åˆ° LoRA adapterï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")

llm.eval()

# ----------------------------
# Image -> prefix embedding
# ----------------------------
class ImageToHidden(torch.nn.Module):
    def __init__(self, clip_dim, hidden_size, prefix_tokens=1):
        super().__init__()
        self.proj = torch.nn.Linear(clip_dim, hidden_size * prefix_tokens)
        self.prefix_tokens = prefix_tokens

    def forward(self, clip_feats):
        out = self.proj(clip_feats)
        out = torch.tanh(out)
        B = out.shape[0]
        hidden_size = out.shape[1] // self.prefix_tokens
        out = out.view(B, self.prefix_tokens, hidden_size)
        return out

# ----------------------------
# æ¨ç†å‡½æ•°
# ----------------------------
def infer(image_path, prompt):
    # è¯»å–å›¾ç‰‡
    image = Image.open(image_path).convert("RGB")
    img_inputs = clip_processor(images=image, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        clip_feats = clip_model.get_image_features(**img_inputs)
        clip_feats = clip_feats.to(llm.dtype)

    # åŠ¨æ€è·å– CLIP è¾“å‡ºç»´åº¦
    clip_dim = clip_feats.shape[1]
    image_to_hidden = ImageToHidden(clip_dim, llm.config.hidden_size, prefix_tokens=PREFIX_TOKENS).to(DEVICE).to(llm.dtype)
    prefix_embeds = image_to_hidden(clip_feats)  # [1, prefix, hidden]

    # æ„é€ è¾“å…¥ token
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(DEVICE)
    attention_mask = torch.ones_like(input_ids).to(DEVICE)

    # æ‹¼æ¥ prefix_embeds
    with torch.no_grad():
        outputs = llm.generate(
            input_ids=None,
            inputs_embeds=torch.cat([prefix_embeds, llm.get_input_embeddings()(input_ids)], dim=1),
            attention_mask=torch.cat([torch.ones(prefix_embeds.size()[:-1], device=DEVICE), attention_mask], dim=1),
            max_new_tokens=200,
            do_sample=True,
            temperature=0.7,
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# ----------------------------
# æ‰§è¡Œ
# ----------------------------
if __name__ == "__main__":
    result = infer(image_path, prompt)
    print("\n=== Generated Answer ===")
    print(result)
